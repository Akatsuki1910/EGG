主なパフォーマンス指標の説明に続き、信頼性確保のための設計に進みます
単一障害点を回避するために、データの複製と複数のVMインスタンスの作成を行います
デプロイの単位を定義して、その能力を把握することが重要です
単一障害点を回避するには、２つの追加インスタンス（N+2）をデプロイして障害とアップグレードに対処します
ゾーン障害を軽減するため、デプロイは別々のゾーンで行ってください
アップグレードの考慮事項を説明します
N+2の実現のために負荷分散される、３台のVMについて考えます
１台のアップグレード中、別の１台で障害が生じると利用可能な演算容量の50%が除かれ、残りのインスタンスの負荷が倍増しそこでも障害が生じる可能性が高まります
ゆえに、キャパシティプランニングを行い、デプロイ単位の能力を把握することが重要です
簡単にスケーリングできるよう、デプロイ単位を相互交換可能なステートレスクローンにすることがおすすめです
関連する項目で同時に障害が生じた場合に起こる相関障害に注意することも重要です
単純なレベルでは、１台のマシンで障害が生じるとそのマシンが処理する全リクエストが失敗します
ハードレベルでは、トップオブラックスイッチで障害が生じるとラック全体がダウンします
クラウドレベルでは、ゾーンまたはリージョンが喪失すると全リソースが利用不可になります
同じSWを実行するサーバーでは、同じ問題が生じます
SWに不具合があると、複数のサーバーで同時期に障害が起こり得ます
相関障害は構成データにも影響する場合があります
グローバル構成システムで障害が発生すると、これを利用する複数のシステムでも障害が起こり得ます
同時に障害が起こる可能性のある一連の関連項目を、障害発生ドメインまたはフォールトドメインといいます
相関障害はいくつかの手法で回避できます
障害発生ドメインを把握しておけば、複数の障害発生ドメインに分散されたマイクロサービスを使いサーバーを分離できます
この場合、障害発生ドメインに基づいてビジネスロジックを複数のサービスに分割し、複数のゾーン/リージョンにデプロイします
より詳細なレベルでは、役割を細分化して複数のプロセスに分散させることをおすすめします
こうすると１つの要素で障害が発生しても、他の要素に影響しません
すべての役割を１つの要素に割り当てると、１つの役割が機能しないだけで全役割が機能しなくなる可能性が高まります
マイクロサービスの設計の際は、独立した疎結合の状態で連携するサービスを設計する必要があります
１つのサービスの障害が他のサービスの障害を導かないようにします
連携サービスの機能低下や、ワークフローを完全には処理できないなどの可能性はありますが、連携サービスは制御可能な状態のままで障害は発生しません
カスケード障害は、１つのシステムの障害により他のシステムが過負荷になることで起こる連鎖的障害です
バックエンドで障害が起き、キュー内のメッセージを処理できずキューが過負荷になったときなどです
左の図ではCloudロードバランサが２つのバックエンドサーバーに負荷分散しています
各サーバーが処理できる秒間クエリ数は最大1,000です
ロードバランサは現在、各インスタンスに毎秒600クエリを送信しています
この状態でサーバーBに障害が起きると、秒間クエリ数1,200をすべてサーバーAのみに送信せざるを得ません
これは指定された最大数より大幅に多いため、カスケード障害を招きかねません
どうすれば回避できるでしょう
カスケード障害には、デプロイプラットフォームで対処できます
Compute Engineのヘルスチェック、GKEのReadiness/Liveness Probeなどを使い、異常なインスタンスを検出、修復できます
新しいインスタンスが迅速に起動することを確認します
他のバックエンドやシステムに依存せずに、起動して準備できれば理想的です
この図は、ロードバランサの背後に４台のサーバーがあるデプロイを示します
現在のトラフィックでは、右のように１台のサーバーで障害が発生しても残りの３台で対処できます
インスタンスグループと自動修復が設定されたCompute Engineでは、障害発生サーバーが新規インスタンスで置換されます
新しいサーバーが迅速に起動し、なるべく早くフル稼働に戻ることが重要です
この設定は、ステートレスサービスでのみ有効です
サービスへのリクエストが障害発生を招く“死のクエリ”の対策も計画する必要があります
“死のクエリ”と呼ばれる理由は、リソースの過剰消費というエラーが出ますが実際の原因はビジネスロジック自体のエラーであるためです
これは診断が難しく、問題の根本原因の特定には適切なモニタリング、オブザーバビリティ、ロギングが必要です
リクエスト時に、レイテンシ、リソース使用率、エラー率をモニタリングしておくと問題特定に役立ちます
問題防止策が問題を招く“正のフィードバックサイクルの過負荷障害”も対策を考える必要があります
これは、障害時の再試行回数を増やすことでシステムの信頼性向上を図った場合に発生します
障害が修正されるどころか過負荷を招く可能性や、すでに負荷が高いシステムにさらに負荷をかける場合があります
解決策は障害発生中のサービスのフィードバックを使ったインテリジェントな再試行です
その対処法を２つ紹介します
障害発生時、再試行するのはかまいませんが制御できる状態で行う必要があります
１つは指数バックオフパターンを使う方法で再試行をすぐには行いません
障害サービスが復旧する猶予時間を確保するため、再試行の間隔をあけ、リクエストが失敗するたびに待機時間を延ばします
再試行回数に上限を設け、断念するまでの時間も制限する必要があります
サービスに対するリクエストが失敗した例を考えてみましょう
指数バックオフを使用し、１秒と数ミリ秒待ってから再試行します
リクエストが再び失敗した場合は、２秒と数ミリ秒待ってから再試行します
再度失敗したら４秒と数ミリ秒待って再試行し、上限に到達するまでこれを繰り返します
回路ブレーカーパターンを使って、過剰な再試行を防ぐこともできます
このパターンは、サービスが縮退稼働状態の際のソリューションを実装します
これは重要です
サービスがダウンするか過負荷状態になり、全クライアントが再試行すると追加リクエストにより状況が悪化するからです
この設計パターンは、サービスの状態を監視するプロキシの背後のサービスを保護します
正常でないと見なされたサービスには、リクエストが転送されません
サービスが稼働状態に戻ると、回路ブレーカーは制御された方法でリクエスト送信を再開します
GKEでは、Istioサービスメッシュによって回路ブレーカーが自動で実装されます
遅延削除は、ユーザーが誤ってデータを削除しても確実に復元できるようにする手段です
遅延削除を使うと、図のように削除パイプラインが開始されて段階的に削除が進行します
最初の段階では、データを削除しても事前定義された期間内であれば復元できます
この例では30日です
これにより、ユーザーのミスからデータを保護します
事前定義の期間が過ぎると、ユーザーがデータを確認することはできなくなり、削除（復元可能）段階に移行します
この段階のデータはユーザーサポートまたは管理者が復元できます
これにより、アプリでのミスからデータを保護します
15日、30日、45日、50日などの復元可能期間が過ぎると、データは削除され使用できなくなります
バックアップやアーカイブが行われていなければ復元できません
