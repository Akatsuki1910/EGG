このラボではHTTPロードバランサを構成し、バックエンドをus-central1とeurope-west1に配置しました
続いてVMを使ったストレステストで、グローバルロードバランシングと自動スケーリングを実証しました
これからラボのチュートリアルを開始しますが、GCPのUIは変更されることがあるので、実際の環境と表示が異なる場合があります
GCP Consoleを表示しています
まず、HTTPとヘルスチェック用のファイアウォールルールを構成します
[VPCネットワーク]の[ファイアウォール]に移動すると、作成済みのファイアウォールルールがいくつかあります
ICMP、内部、RDP、SSHのトラフィック用で、デフォルトネットワークで常に提供されるものです
ここではHTTPを許可するルールを追加するので、[ファイアウォールルールを作成]をクリックします
名前を入力して、デフォルトネットワークに適用します
ターゲットタグを“http-server”に指定します
このターゲットタグを、後でインスタンスに定義します
送信元のIP範囲は、任意の送信元に設定します
それから、TCPを指定します
ポートはHTTP用の80にします
[作成]をクリックします
次に、同様のファイアウォールルールをヘルスチェッカー用に作成します
先ほどのルールが作成されている間に作成しましょう
適用するネットワークは同じです
ターゲットタグも同じにします
こうすると、同じタグがあるインスタンスにルールが適用されます
IP範囲は先ほどより具体的に指定します
ラボの手順に記載されていますが、これらはヘルスチェッカーのIP範囲です
入力するときは、まず１つの範囲を入力します
空白箇所をクリックすると確定されるので、次にもう一方のIP範囲をコピーして貼り付け、同様に空白箇所をクリックすると、これも確定されます
プロトコルとポートには、TCPで“all”と指定しますが、ヘルスチェックの種類によっては絞り込むこともできます
[作成]をクリックします
ルールが作成されている間に、カスタムイメージを作成するので、[Compute Engine]に移動します
ここで作成するのは、VMです
名前は“webserver”にします
リージョンはus-central1のままにします
ゾーンはus-central1-aです
このオプションを展開します
[ネットワーキング、ディスク、セキュリティ、管理、単一テナンシー]です
最初に、[ディスク]でインスタンスの削除時にブートディスクを維持するように設定します
これらはネットワークにアタッチされる永続ディスクなので問題ありません
[ネットワーキング]では、ネットワークタグの“http-server”を定義します
適用対象はデフォルトネットワークです
先ほど作成したファイアウォールが、このインスタンスに適用されます
[作成]をクリックします
インスタンスが稼働中になったら、ソフトをインストールしてカスタマイズします
インスタンスが作成されるまで待ちます
[SSH]をクリックします
次に、ラボの手順に記載されているコマンドを実行します
まず、Apache2をインストールします
次にApacheサーバーを起動してから、サーバーをダブルチェックするためにここにある外部IPアドレスに移動します
そのために外部IP用のファイアウォールルールを追加しました
ヘルスチェック用のルールはまだ必要ありません
ヘルスチェックは後で作成するバックエンドインスタンスに使用するので、まだ構成する必要はありません
まだ接続中です
あと数秒待ちましょう
接続完了です
２つのコマンドをここに貼り付けて、実行します
次に、サービスを開始します
Consoleに戻って外部IPをクリックします
Apache2 Debianのデフォルトページが表示されたので機能しています
次は、起動時にサービスを開始するよう、設定するコマンドを実行します
SSHターミナルに戻ります
そのコマンドを貼り付けます
Compute Engineに戻ります
webserverを選択して、[リセット]をクリックします
確認メッセージが表示されるので、[リセット]をクリックします
これでマシンが停止して再起動します
同じIPアドレスと永続ブートディスクを維持しますが、メモリはワイプされます
そのためApacheサービスが使用可能になるのは、リセット後にupdateまたはcコマンドが成功した後です
それまで待ちましょう
ステータスを確認するには、外部IPアドレスに移動するか、バックアップ完了後にSSHでインスタンスに再接続し、ステータスを確認するコマンドを実行します
出力から、Apacheサービスが実行中であることがわかります
次はディスクを準備して、そこからカスタムイメージを作成します
まずSSHセッションを終了します
次に、ディスクが関連付けられたインスタンスを削除した場合に、ディスクが削除されないことをもう一度確認します
これを確認するには、インスタンスの名前をクリックします
下にスクロールして、ブートディスクのセクションを見つけます
[インスタンスを削除したときの動作]を確認します
[ディスクを維持]になっていなければ、[編集]をクリックして動作を変更します
この場合は変更不要なので、インスタンスを削除します
ディスクも削除するかどうか尋ねられますが、この場合は削除しません
インスタンスを削除して、[ディスク]に移動すると、ディスク自体は残っていることを確認できます
これで、インスタンスを削除してもディスクが維持されることを確認できました
次はイメージを作成するので、[イメージ]をクリックします
使用可能なイメージが表示されますが、ここでは、カスタムイメージを作成します
名前は“mywebserver”にします
これはソースディスクとして使いますが、ほかにもスナップショット、別のディスク、Storageファイルをソースとして選べます
使用可能なディスクは１つしかないので、それをソースとして選択します
暗号化、ロケーションなどの設定はすべてそのままにして[作成]をクリックします
これで、選択したディスクからイメージが作成されます
イメージが作成されたら、ディスク自体を削除することもできます
ディスクが存在する間は、料金が発生するためですが、このラボでは削除する必要はありません
どのQwiklabsプロジェクトでも、使用したリソースはクリーンアップされます
次はインスタンステンプレートを構成して、インスタンスグループを作成しましょう
[Compute Engine]の[インスタンステンプレート]に移動します
新しいインスタンステンプレートを作成して、名前を付けます
“mywebserver-template”です
マシンタイプを変更して“micro”にします
小規模なプロトタイピングにはこれで十分です
ここで重要なのはブートディスクを変更して、カスタムイメージを選択することです
変更するために、[カスタムイメージ]に移動します
これがこのプロジェクトのカスタムイメージです
他のプロジェクトにアクセスできれば、そこからもイメージを取得できます
ディスクのサイズや種類も選択できますが、ここでは設定を変えずに、[選択]をクリックします
正しいネットワークタグも設定する必要があります
[ネットワーキング、ディスク...]を展開します
ちなみに、このインスタンステンプレートのUIはVMインスタンステンプレートに似ています
VMのルールを定義しているだけだからです
VMをグループ化すると、全設定がグループに適用されます
ここで[ネットワーキング]に移動して、ネットワークを選択します
この場合は、デフォルトネットワークであることを確認してからネットワークタグを設定し、冒頭で作成したルールがこのテンプレートで作成される全VMに適用されるようにします
[作成]をクリックします
インスタンスではなくテンプレートが作成されるだけなので時間はかからないはずです
待てない場合は、[更新]をクリックします
作成されました
これで[インスタンスグループ]に移動して、インスタンスグループを作成できます
まず、インスタンスグループをus-central1に作成します
マルチゾーンを選択して、リージョングループにします
us-central1全体のグループです
ここをクリックすると、ゾーンの選択解除や追加を行えます
先ほど作成したテンプレートに基づくようにします
重要な設定として[自動スケーリングモード]をオンにして自動スケーリングを有効にし、[HTTPロードバランシングの使用率]をその指標にします
ポートは80、インスタンスの最小数は１、最大数は５にします
クールダウン期間はそのままにします
これはインスタンスの情報を収集するまで、待機する時間です
このインスタンスには初期化が必要なので、少なくても60秒待機してから、情報収集が開始されるようにします
ヘルスチェックはまだありませんが、ここで作成できます
“http-health-check”という名前にします
プロトコルにはHTTPを選ぶか、TCP、80のままにできます
この構成だと10秒ごとにチェックが行われ、レスポンスが５秒待機されます
２回連続で成功すると正常とみなされ、３回連続して失敗するとインスタンスは異常とみなされます
[保存]をクリックします
[初期遅延]は起動するための待機時間で、これは60秒に設定します
[作成]をクリックします
自動スケーリング構成が未完了とあるのは、HTTPロードバランシングが未設定だからです
これから設定するので[OK]をクリックします
同じ作業を繰り返してeurope-west1にインスタンスグループを作成します
名前を変更します
これもマルチゾーンにします
この場合のリージョンは、europe-west1です
同じインスタンステンプレートを選び、自動スケーリングの指標も同じくHTTPで、ポート80、最小数１、最大数５を設定し、クールダウン期間も同じにします
ヘルスチェックは、選択するだけで済むはずですが、まだ表示されていません
作業ペースが速すぎるとこうなります
[キャンセル]をクリックして、前のインスタンスグループが作成されたことを確認しましょう
もう一度試してみると、今回はヘルスチェックが表示されました
作業ペースが速いと、こうしたことが起こります
最初からやり直します
名前をここに貼り付けて、マルチゾーンを選び、europe-west1を選択します
テンプレートは作成するのでなく、作成済みのものを選択します
HTTP 最大数は５、初期遅延は同じく60に設定して、ラボでは待機時間を短くします
これで、[作成]をクリックすると、同じ警告が表示されますが、[OK]をクリックします
このインスタンスグループが、作成中になりました
[VMインスタンス]に移動すると、一方のインスタンスグループが、その名前で始まるインスタンスを作成済みであることを確認できます
ちなみに“mig”はマネージドインスタンスグループの略です
スケーリングも確認できます
これはインスタンスが１つ、これは０から１にスケールしています
名前をクリックすると詳細を確認できます
[モニタリング]に移動すると、CPU利用率が示され、[詳細]、[概要]タブでもスケーリングやメンバー数を確認できます
[インスタンスグループ]ページでも[VMインスタンス]でも豊富な情報を入手できます
いずれにしても各グループで、１つのインスタンスが作成されたので、次に構成するのはバックエンドです
これらのインスタンスを実際に確認するには、ナビゲーションメニューで[VMインスタンス]に移動して、それぞれのIPアドレスにアクセスします
どちらもクリックすると、デフォルトページが表示されます
前に作成したカスタムイメージが実際に使われている証拠です
インストールしたカスタムソフトがバックエンドで使用されています
次はHTTPロードバランサを構成しましょう
ナビゲーションメニューで、[ネットワークサービス]の[Cloud Load Balancing]に移動して、ロードバランサを作成します
HTTPロードバランサの構成を開始します
インターネット接続か内部専用を選べるので、[インターネットからVM...]を選択して、[続行]をクリックします
名前は“http-lb”にします
最初にバックエンドを構成します
バックエンドサービスを作成して、名前を入力します
インスタンスグループとして、まずはus-central1を選択します
ポート番号は80です
バランシングモードは“レート”にします
１秒あたりの最大レート（RPS）は“50”、容量は“100”にしてラボの手順に従います
この構成ではロードバランサが、各インスタンスのRPSを50以下に維持しようとします
[完了]をクリックして、残っているもう一方のバックエンドを追加します
ここでは、[使用率]を選択して、最大CPU使用率を“80”にします
容量は“100”にします
つまり、この構成ではロードバランサがeurope-west1の各インスタンスのCPU使用率を80パーセント以下に維持しようとします
ここでも同じヘルスチェックを追加してから、[作成]をクリックします
ホストとパスのルールも構成できます
特定のトラフィックをURLに応じて、他のバックエンドに転送するように設定できます
たとえば動画サービスを動画バックエンドに転送し、静的コンテンツを静的バックエンドに転送するなどです
ここでは省略して、フロントエンドの構成に進みます
名前も付けられますが、指定する必要があるのはプロトコルとIPバージョンです
IPアドレスはエフェメラル、ポートは80で[完了]をクリックします
確認と完了に進み、バックエンドを確認し、インスタンスグループを確認します
フロントエンドも確認します
必要に応じて、ここに戻って別のフロントエンドを追加します
HTTPプロトコルを選択し、IPv6も追加できるので、これを追加します
２つのフロントエンドで２つのIPアドレスを取得できます
この構成で作成しましょう
稼働中になると２つのアドレスが表示されるはずです
そのうち、16進数形式のアドレスがIPv6アドレスです
このアドレスには許可されている場所からでしか移動できません
スマートフォンでは通常、IPv6が使われているので、スマートフォンでこのアドレスに接続すればこれらのバックエンドにアクセスできるでしょう
読み込まれるまで待ちましょう
ロードバランサをクリックしてみます
フロントエンドはまだ準備中です
早すぎたようです
画面を更新します
もう少し待ってサービスが準備できたら、サービスに接続して詳細情報を入手できます
このとおり、ロードバランサが設定されました
更新してから数秒で完了しました
ここにIPアドレスが示されています
これがIPv4で、これがIPv6です
これらのIPアドレスには、ブラウザを使って移動できます
任意の場所からのHTTPトラフィックを許可したからです
ブラウザにアドレスを入力して、最初にIPv4に移動すると、ラボのマニュアルに記載されているとおり404エラーを受け取ります
別のタブを開いてIPv6アドレスも入力してみます
どうなるでしょうか
サービスが見つからないとあります
ラボのマニュアルによると、しばらくの間は404または502エラーが返されます
何度か更新してみてください
何を待機しているのかというと、この構成がすべてのGoogleフロントエンドに適用された状態になることです
これは世界中で適用されるグローバルロードバランサです
そのため実際の実装ではConsoleですべて準備できたように見えても、サービスに到達可能になるには時間がかかることがあります
設定にかかる時間は数分なので、更新しながら完了するのを待ちましょう
これは、何度か更新して表示されたIPv4アドレスのページです
バックエンドはApache2 Debianのデフォルトページです
IPv6アドレスにも移動してみましょう
アクセスできました
つまり想定どおりに機能しています
バックエンドの動作を確認したので、次はストレステストを行います
まず別のインスタンスを作成し、そこから大量のトラフィックをロードバランサに送信して、そのトラフィックをモニタリングします
別のタブを開いて、このロードバランサに戻れるようにしておきます
別のインスタンスを作成するために、[Compute Engine]に戻ります
[インスタンスを作成]をクリックします
名前は“stress-test”にします
配置先はまったく別のリージョンにします
us-west1を選択します
バックエンドはus-central1とeurope-west1に、それぞれ１つあります
この新規インスタンスから最も近いバックエンドはus-central1にあるので、トラフィックはus-westからus-centralに転送されるはずです
ただし負荷が高すぎる場合は別です
そこで、かなり高い負荷をかけて、トラフィックがヨーロッパリージョンにスピルオーバーされるかどうか確認します
ブートディスクを変更します
前に作成したカスタムイメージを選択します
これで一連のソフトがプリインストールされます
これを作成します
これが稼働中になったら、ロードバランサのIPアドレスを環境変数に格納して、そこから取得できることを確認します
その後、ロードバランサに負荷をかけます
インスタンスが起動するのを待ちましょう
どの新規プロジェクトでも、右側のパネルに詳しい情報が表示されます
GCPに不慣れな場合は、ここで情報を確認するとよいでしょう
インスタンスが起動したので、[SSH]をクリックします
次は、環境変数に格納するIPアドレスを取得するので、ここに戻ります
IPv4アドレスを使用します
次に取得するのは、ストレステストのバックアップです
これも格納します
念のために確認します
返されるIPアドレスとこのIPアドレスは、一致しています
次は、ロードバランサに負荷をかけるコマンドを実行しましょう
ApacheBenchを使用して、ベンチマークを行います
これはバックグラウンドで実行されます
その間にできることとして、ここに表示されているロードバランサに戻ります
こうすることで、バックエンドを直接確認できます
http-backendをクリックします
まだトラフィックは生成されていません
表示が更新されるまでしばらくかかります
２つのバックエンドがあり、一方はレートを基にスケールし、もう一方はCPU使用率を基にスケールします
トラフィックが大量になると、トラフィックの送信元と送信先インスタンスがここに表示されるようになります
それまで、このページを表示したまま待ちます
数分してページを更新すると、生成されているトラフィックを実際に確認できるはずです
このページに戻ります
トラフィックはまだないので、１、２分待ちましょう
何が表示されるでしょうか
数秒で表示が変わりました
北米から送信されている大量のトラフィックは、ストレステストによるものです
大半のリクエストのトラフィックは最も近いus-central1に送信されていますが、一部のトラフィックはeurope-west1のインスタンスにスピルオーバーしています
つまりグローバルロードバランシングが行われています
バックエンドをモニタリングして、実際にスケールしているかも確認できます
Compute Engineに移動して更新すると、すでにバックエンドの数が増えています
これらでトラフィックの急増に対処しています
かなり大々的なストレステストです
[インスタンスグループ]に移動して、エラーを調べると、選択した最大インスタンス数５では問題があるとあります
europe-west1の[詳細]タブで詳細を確認できます
[モニタリング]タブにはどのようにスケールアップしたかと、どのように負荷に対処しているかが示されます
us-central1についても確認しましょう
すでに最大数５つのインスタンスがさまざまなゾーンに作成されています
[モニタリング]の詳細には、いつスケールアップしたか示されています
少し時間をずらして更新すると、表示されるインスタンスが増え、インスタンスの容量も確認できます
このページに戻ります
us-central1への送信トラフィックは最小限であり、50 RPSだけでしたが、281まで増やしたので大量のトラフィックがスピルオーバーしています
このビューはロードバランサの継続的モニタリングに非常に役立ちます
可視化ツールにはStackdriver、Logging、Monitoringもあり、アラートやルールも設定できます
おそらく、最大数の設定を５より大きくする必要がありますが、実はこれは費用を使い過ぎないよう制限するための設定です
しかしトラフィックへの対処が必要であればインスタンスを増やせます
攻撃を受けている場合は、Cloud Armorというプロダクトを使って、特定のIPアドレスを許可または拒否できます
これでラボのすべての作業が完了しました
